{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2797b964-c05b-40be-913d-a2913a93d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bd13bb-f30b-43b5-a8fd-bd0174ca1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 8 * 8, 4)  # 4 classes for rotation\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baae98c2-9ed8-49a9-866e-c5cec24b0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(x):\n",
    "    angles = [0, 90, 180, 270]\n",
    "    angle = torch.randint(0, 4, (1,), dtype=torch.int64).item()\n",
    "    return transforms.functional.rotate(x, angles[angle]), angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908dcd36-24bb-488e-9a46-99497dc42819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRotatedCIFAR10(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.dataset = original_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.dataset[index]\n",
    "        rotated_img, label = rotate_image(img)\n",
    "        return rotated_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c922ef7-d07c-4b33-b739-18854a47697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "original_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "rotated_dataset = CustomRotatedCIFAR10(original_dataset)\n",
    "train_loader = DataLoader(rotated_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36b6904-bfb9-455f-880f-bb4b5d0f4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e699b5cb-ac44-4dee-a226-4cd1e0cfde14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mushfika_rahman1\\AppData\\Local\\Temp\\ipykernel_10552\\1411649858.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)  # Convert labels to a tensor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1563], Loss: 1.3036\n",
      "Epoch [1/5], Step [200/1563], Loss: 1.2039\n",
      "Epoch [1/5], Step [300/1563], Loss: 1.1686\n",
      "Epoch [1/5], Step [400/1563], Loss: 1.1820\n",
      "Epoch [1/5], Step [500/1563], Loss: 1.1551\n",
      "Epoch [1/5], Step [600/1563], Loss: 1.1260\n",
      "Epoch [1/5], Step [700/1563], Loss: 1.1189\n",
      "Epoch [1/5], Step [800/1563], Loss: 1.1057\n",
      "Epoch [1/5], Step [900/1563], Loss: 1.0804\n",
      "Epoch [1/5], Step [1000/1563], Loss: 1.0986\n",
      "Epoch [1/5], Step [1100/1563], Loss: 1.0784\n",
      "Epoch [1/5], Step [1200/1563], Loss: 1.0790\n",
      "Epoch [1/5], Step [1300/1563], Loss: 1.0450\n",
      "Epoch [1/5], Step [1400/1563], Loss: 1.0521\n",
      "Epoch [1/5], Step [1500/1563], Loss: 1.0606\n",
      "Epoch [2/5], Step [100/1563], Loss: 1.0406\n",
      "Epoch [2/5], Step [200/1563], Loss: 1.0568\n",
      "Epoch [2/5], Step [300/1563], Loss: 1.0166\n",
      "Epoch [2/5], Step [400/1563], Loss: 1.0101\n",
      "Epoch [2/5], Step [500/1563], Loss: 1.0332\n",
      "Epoch [2/5], Step [600/1563], Loss: 1.0250\n",
      "Epoch [2/5], Step [700/1563], Loss: 1.0119\n",
      "Epoch [2/5], Step [800/1563], Loss: 1.0548\n",
      "Epoch [2/5], Step [900/1563], Loss: 1.0262\n",
      "Epoch [2/5], Step [1000/1563], Loss: 1.0177\n",
      "Epoch [2/5], Step [1100/1563], Loss: 1.0069\n",
      "Epoch [2/5], Step [1200/1563], Loss: 0.9852\n",
      "Epoch [2/5], Step [1300/1563], Loss: 1.0154\n",
      "Epoch [2/5], Step [1400/1563], Loss: 1.0032\n",
      "Epoch [2/5], Step [1500/1563], Loss: 0.9788\n",
      "Epoch [3/5], Step [100/1563], Loss: 1.0047\n",
      "Epoch [3/5], Step [200/1563], Loss: 0.9742\n",
      "Epoch [3/5], Step [300/1563], Loss: 0.9597\n",
      "Epoch [3/5], Step [400/1563], Loss: 0.9717\n",
      "Epoch [3/5], Step [500/1563], Loss: 0.9739\n",
      "Epoch [3/5], Step [600/1563], Loss: 0.9930\n",
      "Epoch [3/5], Step [700/1563], Loss: 0.9835\n",
      "Epoch [3/5], Step [800/1563], Loss: 0.9819\n",
      "Epoch [3/5], Step [900/1563], Loss: 1.0070\n",
      "Epoch [3/5], Step [1000/1563], Loss: 1.0066\n",
      "Epoch [3/5], Step [1100/1563], Loss: 0.9691\n",
      "Epoch [3/5], Step [1200/1563], Loss: 0.9529\n",
      "Epoch [3/5], Step [1300/1563], Loss: 0.9659\n",
      "Epoch [3/5], Step [1400/1563], Loss: 0.9806\n",
      "Epoch [3/5], Step [1500/1563], Loss: 0.9920\n",
      "Epoch [4/5], Step [100/1563], Loss: 0.9707\n",
      "Epoch [4/5], Step [200/1563], Loss: 0.9383\n",
      "Epoch [4/5], Step [300/1563], Loss: 0.9564\n",
      "Epoch [4/5], Step [400/1563], Loss: 0.9581\n",
      "Epoch [4/5], Step [500/1563], Loss: 0.9569\n",
      "Epoch [4/5], Step [600/1563], Loss: 0.9322\n",
      "Epoch [4/5], Step [700/1563], Loss: 0.9439\n",
      "Epoch [4/5], Step [800/1563], Loss: 0.9586\n",
      "Epoch [4/5], Step [900/1563], Loss: 0.9807\n",
      "Epoch [4/5], Step [1000/1563], Loss: 0.9314\n",
      "Epoch [4/5], Step [1100/1563], Loss: 0.9437\n",
      "Epoch [4/5], Step [1200/1563], Loss: 0.9638\n",
      "Epoch [4/5], Step [1300/1563], Loss: 0.9679\n",
      "Epoch [4/5], Step [1400/1563], Loss: 0.9632\n",
      "Epoch [4/5], Step [1500/1563], Loss: 0.9383\n",
      "Epoch [5/5], Step [100/1563], Loss: 0.9526\n",
      "Epoch [5/5], Step [200/1563], Loss: 0.9395\n",
      "Epoch [5/5], Step [300/1563], Loss: 0.9245\n",
      "Epoch [5/5], Step [400/1563], Loss: 0.9169\n",
      "Epoch [5/5], Step [500/1563], Loss: 0.9351\n",
      "Epoch [5/5], Step [600/1563], Loss: 0.9260\n",
      "Epoch [5/5], Step [700/1563], Loss: 0.9602\n",
      "Epoch [5/5], Step [800/1563], Loss: 0.9487\n",
      "Epoch [5/5], Step [900/1563], Loss: 0.9336\n",
      "Epoch [5/5], Step [1000/1563], Loss: 0.9402\n",
      "Epoch [5/5], Step [1100/1563], Loss: 0.9085\n",
      "Epoch [5/5], Step [1200/1563], Loss: 0.9477\n",
      "Epoch [5/5], Step [1300/1563], Loss: 0.9541\n",
      "Epoch [5/5], Step [1400/1563], Loss: 0.9252\n",
      "Epoch [5/5], Step [1500/1563], Loss: 0.9230\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.stack([image for image in images])  # Stack images into a single tensor\n",
    "        labels = torch.tensor(labels)  # Convert labels to a tensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'rotation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28560fbf-88d9-4658-bdf1-d6aaab6f1d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_model = simple_cnn()\n",
    "rotation_model.load_state_dict(torch.load('rotation_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fa67d8-7bc3-439f-a4ef-2001b0eb100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_model[-1] = nn.Linear(32 * 8 * 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fff726-1052-4393-8954-36c09f5ceae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d904f8bc-46e1-45e0-946b-7b4bd74f03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a87129-6f23-446b-90ae-037e1fc5c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/391], Loss: 2.5605\n",
      "Epoch [1/10], Step [200/391], Loss: 2.5610\n",
      "Epoch [1/10], Step [300/391], Loss: 2.5647\n",
      "Current epoch [1/10], accuracy: 9.6040\n",
      "Epoch [2/10], Step [100/391], Loss: 2.5705\n",
      "Epoch [2/10], Step [200/391], Loss: 2.5567\n",
      "Epoch [2/10], Step [300/391], Loss: 2.5455\n",
      "Current epoch [2/10], accuracy: 9.7140\n",
      "Epoch [3/10], Step [100/391], Loss: 2.5521\n",
      "Epoch [3/10], Step [200/391], Loss: 2.5537\n",
      "Epoch [3/10], Step [300/391], Loss: 2.5619\n",
      "Current epoch [3/10], accuracy: 9.7340\n",
      "Epoch [4/10], Step [100/391], Loss: 2.5630\n",
      "Epoch [4/10], Step [200/391], Loss: 2.5507\n",
      "Epoch [4/10], Step [300/391], Loss: 2.5507\n",
      "Current epoch [4/10], accuracy: 9.6360\n",
      "Epoch [5/10], Step [100/391], Loss: 2.5493\n",
      "Epoch [5/10], Step [200/391], Loss: 2.5640\n",
      "Epoch [5/10], Step [300/391], Loss: 2.5603\n",
      "Current epoch [5/10], accuracy: 9.6200\n",
      "Epoch [6/10], Step [100/391], Loss: 2.5686\n",
      "Epoch [6/10], Step [200/391], Loss: 2.5605\n",
      "Epoch [6/10], Step [300/391], Loss: 2.5493\n",
      "Current epoch [6/10], accuracy: 9.5840\n",
      "Epoch [7/10], Step [100/391], Loss: 2.5613\n",
      "Epoch [7/10], Step [200/391], Loss: 2.5526\n",
      "Epoch [7/10], Step [300/391], Loss: 2.5567\n",
      "Current epoch [7/10], accuracy: 9.6120\n",
      "Epoch [8/10], Step [100/391], Loss: 2.5519\n",
      "Epoch [8/10], Step [200/391], Loss: 2.5607\n",
      "Epoch [8/10], Step [300/391], Loss: 2.5532\n",
      "Current epoch [8/10], accuracy: 9.4880\n",
      "Epoch [9/10], Step [100/391], Loss: 2.5530\n",
      "Epoch [9/10], Step [200/391], Loss: 2.5680\n",
      "Epoch [9/10], Step [300/391], Loss: 2.5604\n",
      "Current epoch [9/10], accuracy: 9.7540\n",
      "Epoch [10/10], Step [100/391], Loss: 2.5645\n",
      "Epoch [10/10], Step [200/391], Loss: 2.5594\n",
      "Epoch [10/10], Step [300/391], Loss: 2.5489\n",
      "Current epoch [10/10], accuracy: 9.6000\n",
      "Finished Training on original labels\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "rotation_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rotation_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    accuracy = 100*total_correct / total_samples\n",
    "    train_accuracies.append(accuracy)\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    print(f'Current epoch [{epoch+1}/{num_epochs}], accuracy: {accuracy:.4f}')\n",
    "    \n",
    "print('Finished Training on original labels')\n",
    "# Save the model\n",
    "torch.save(rotation_model.state_dict(), 'cifar10_classifier_from_rotation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05afa2e4-922e-41bb-a255-f87b072cb391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the CIFAR-10 train images: 12.22%\n"
     ]
    }
   ],
   "source": [
    "#rotation_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        outputs = rotation_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the CIFAR-10 train images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25c9dcfb-d046-4f81-adc0-5851ba20027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the CIFAR-10 test images: 12.44%\n"
     ]
    }
   ],
   "source": [
    "#rotation_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = rotation_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the CIFAR-10 test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f2d90-2979-48a6-a0f7-8b04fd563b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(train_losses, label=\" Trainning Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Trainning Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae56fd9-d884-4fca-bdaa-03a3f066afe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpu",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
